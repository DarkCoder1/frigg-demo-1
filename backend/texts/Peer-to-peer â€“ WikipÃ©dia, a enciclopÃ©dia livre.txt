
Peer-to-peer (do inglês par-a-par ou simplesmente ponto-a-ponto, com sigla P2P) é uma arquitetura de redes de computadores onde cada um dos pontos ou nós da rede funciona tanto como cliente quanto como servidor, permitindo compartilhamentos de serviços e dados sem a necessidade de um servidor central. As redes P2P podem ser configuradas em casa, em Empresas e ainda na Internet. Todos os pontos da rede devem usar programas compatíveis para ligar-se um ao outro. Uma rede peer-to-peer pode ser usada para compartilhar músicas, vídeos, imagens, dados, enfim qualquer coisa com formato digital.
Os Peers são os participantes da rede igualmente privilegiados na aplicação. Essa aplicação tem suas tarefas ou cargas dividas em pares. Cada computador da rede é um nó (ponto de interconexão da rede) e fica responsável por uma parcela dos recursos da rede, tais como armazenamento, poder de processamento e largura de banda. Os recursos são divididos diretamente entre cada participante da rede sem a necessidade de uma coordenação central de um servidor ou hosts. Nesse modelo de rede, cada par de computadores são fornecedores e consumidores de recurso, diferentemente do modelo cliente-servidor, onde o servidor alimenta toda a rede e os clientes somente consomem. Os novos sistemas P2P estão indo além do compartilhamento entre pares, estão buscando pares diferentes que podem trazer recursos, capacitando os pares individuais para realizarem tarefas maiores, mas que são de benefícios de todos os pares.
Esse tipo de arquitetura de rede é muito conhecida pelo compartilhamento de ficheiros. No entanto as redes P2P são utilizadas para outras áreas, tais como, armazenamento distribuídos em meios acadêmico e científico e telecomunicações, por exemplo.


Vantagens:
Nessa arquitetura a partilha de recursos e a performance são superiores. Como os recursos necessários são distribuídos (carga computacional, tráfego de rede, espaço de armazenamento, etc) entre os nós é possível conseguir um melhor desempenho de forma econômica. Do contrário da rede cliente-servidor onde a performance depende do desempenho do servidor.
Devido a inexistência de um servidor central, a rede P2P apresenta uma maior robustez e segurança por não poder sofrer um ataque centralizado.
Arquitetura P2P: Um sistema peer-to-peer implementa uma rede abstrata sobreposta, em cima da topologia da rede. Essa sobreposição é utilizada para descobrir e indexar os pares da rede tornando o sistema P2P funcional independente da topologia da rede física. O conteúdo é trocado diretamente sobre o protocolo IP. Sistema peer-to-peer anônimos são um exceção, implementam camadas extras de roteamento para ocultar sua identidade de origem ou de destino.
A rede Overlay P2P consiste em todos os pares da rede funcionem como um nó da rede. Existe uma ligação entre dois nós que se conhecem na rede. Isto é, um nó participante conhece a localização de outro nó da rede P2P. Então existe uma aresta que liga um primeiro nó existente ao segundo na rede sobreposta. Com base de como os nós estão conectados na rede, podemos classificar a rede P2P como Estruturada ou não Estruturada.
Na estrutura P2P, os pares são organizados de acordo com critérios e algoritmos, que realizam a sobreposição com topologias específicas. Normalmente usam tabelas Hash distribuídas para indexação. Estruturados sistemas P2P são adequados para implementações em larga escala devido à alta escalabilidade e algumas garantias sobre o desempenho (normalmente aproximando O(log N), onde N é o número de nós no sistema P2P).
Não é imposto pela rede P2P uma estrutura padrão para rede Overlay(sobreposta). Na ideologia não existe no sistema de estrutura P2P um elemento centralizador, entretanto na prática existe, em sistemas estruturados, vários graus de centralização. Três categorias se destacam:
Peer-to-peer centralizado: é utilizado um servidor central para indexar as informações e iniciar o sistema inteiro. As conexões entre pares não é gerenciada por qualquer algoritmo.
Peer-to-peer hibridas: permite que os nós da infra-estrutura co-existam.
O primeiro e popular sistema peer-to-peer de compartilhamento de arquivos, o Napster, foi um exemplo do modelo centralizado. Freenet e implementações iniciais do protocolo gnutella, por outro lado, são exemplos do modelo descentralizado. Implementações gnutella modernos, Gnutella2, bem como o agora substituído rede Kazaa são exemplos do modelo híbrido.
A demanda por serviços na Internet vem crescendo a uma escala que só pode ser limitada pelo tamanho da população mundial. Um dos objetivos dos sistemas peer-to-peer é permitir o compartilhamento de dados e recursos numa larga escala excluindo qualquer requisito por servidores gerenciados separadamente e a sua infraestrutura associada. Sistemas peer-to-peer têm o propósito de suportar sistemas e aplicações distribuídas utilizando os recursos computacionais disponíveis em computadores pessoais e estações de trabalho em número crescente. Isso tem se mostrado bastante atrativo, já que a diferença de performance entre desktops e servidores tem diminuído e as conexões de banda larga têm proliferado.
Shirky[1] definiu as aplicações P2P como "aplicações que exploram recursos disponíveis nas bordas da Internet - armazenamento, ciclos, conteúdo, presença humana"..[2]
Geralmente, uma rede peer-to-peer é constituída por computadores ou outros tipos de unidades de processamento que não possuem um papel fixo de cliente ou servidor, pelo contrário, costumam ser considerados de igual nível e assumem o papel de cliente ou de servidor dependendo da transação sendo iniciada ou recebida de um outro par da mesma rede.
Os nós da rede peer-to-peer podem diferir em termos de configuração local, capacidade de processamento, capacidade de armazenamento, largura de banda, entre outras características particulares. O primeiro uso da expressão peer-to-peer foi em 1984, com o desenvolvimento do projeto Advanced peer-to-peer Networking Architecture na IBM.
O termo é utilizado em diferentes tecnologias que adotam um modelo conceptual peer-to-peer (em Portugal, conhecido como par-a-par), tal como o protocolo NNTP (para Usenet News), SMTP (para envio de mensagens eletrônicas - e-mail), e sistemas de troca de mensagens instantâneas (ICQ, MSN). Porém, o termo tornou-se popular com o surgimento de aplicações de compartilhamento de arquivo, em outras palavras, programas que possibilitam a distribuição de arquivos em rede, permitindo o acesso de qualquer usuário dessa rede a este recurso. Outros tipos de recursos podem ser compartilhados em redes Par-a-Par, tal como capacidade de processamento de máquinas, espaço de armazenamento de arquivos, serviços de programas (software, em inglês) - analogamente aos Web Services, entre outros.
Em 1999, Shawn Fanning criou o Napster, para compartilhamento de arquivos de música (principalmente MP3), e trouxe o conceito de Par-a-Par para a mídia, principalmente após tornar-se alvo de ataques jurídicos por parte das companhias fonográficas.
O P2P é o resultado da tendência natural do desenvolvimento de engenharia de software com a disponibilidade de tecnologia para a criação de redes maiores.
A tendência das últimas décadas tem crescido com a necessidade das aplicações empresariais, o que resultou na substituição dos sistemas monolíticos por sistemas distribuídos. As redes informáticas começaram a crescer, tornando-se cada vez maiores e mais poderosas, os utilizadores começaram a cada vez ser mais, a banda larga tornou-se cada vez mais barata e poderosa, sendo fácil de aceder.
Neste campo houve a necessidade de ter a disponibilidade de pontos interligados, e quantos mais recursos houvesse, mais poderosa se tornava essa rede. A Internet foi um claro exemplo e uma explosão de utilizadores.
Enquanto as redes cresciam, as aplicações P2P desenvolveram-se e a sociedade interessava-se pelo P2P. Aplicações como o Napster, Gnutella e Kazaa ficaram famosos, porque estas colocaram um subconjunto da tecnologia P2P que estava ao alcance de milhares de utilizadores.
Na verdade o P2P surgiu da tecnologia básica que utilizaram nos tempos da Usenet e da FidoNet. Eram duas redes totalmente descentralizadas, e sistemas como o DNS.
No início eram trabalhos que dois estudantes tinham na licenciatura. Não existia muita informação sobre a partilha, nem Internet como hoje em dia. Os arquivos eram trocados em batch nas linhas telefónicas, porque os computadores eram ligados por cabos coaxiais, não tendo muito mais que 10 computadores em cada rede, ligando-se por um terminador, passando virtualmente por cada nó.
No entanto em 1979, como não havia maneira de centralizar, a Usenet, era totalmente descentralizada sendo o criador Jim Ellis (será explicado mais à frente o que é). A FidoNet também era descentralizada, mas servia para troca de mensagens. Esta aplicação foi criada em 1984 por Tom Jennings para trocar mensagens de sistemas (BBS) diferentes.
O DNS tornou-se uma necessidade, porque em 1983 já existiam milhões de hosts na Internet.[carece de fontes?] Na altura a forma de navegar na Internet era através de um ficheiro.txt, nomeado de hosts.txt. O nome "cin" era associado um determinado ip do ficheiro hosts.txt. Como a Internet cresceu, este sistema tornou-se impossível e foi então que surgiu o DNS.
O conceito de DNS é comparado às aplicações de ficheiros actuais.
Mas foi na década de 1990 que as redes P2P apareceram com toda a força, quando aplicações como o Napster e o Gnutella foram desenvolvidas. Cada nó neste tipo de rede é conhecido como peer e pode servir com os mesmos direitos de cada peer da rede, serve tanto de cliente como de servidor. Os recursos e as informações passaram a ser disponibilizados de forma mundial. Estas redes tinham características, que quantos mais peers existissem mais estabilidade e mais autonomia tinham, e a rede tornava-se mais eficiente e rica em recursos com a comunicação directa que os peers tinham. Nos dias de hoje as redes de computadores é o que se trata mais salientando as redes ponto-a-ponto pelo seu baixo custo e pela sua fácil implementação no mercado técnico as redes ponto-a-ponto chega a ser um um ganho visto pelo seu nome é uma rede que não tem um gerenciador ou administrador de página.
A organização de uma aplicação cliente-servidor numa arquitetura multi-camadas distribui o processamento colocando componentes logicamente diferentes em máquinas diferentes. A essa distribuição se dá o nome de distribuição vertical. Uma distribuição vertical facilita o gerenciamento dos sistemas pois divide as funções lógica e fisicamente entre várias máquinas, onde cada uma é responsável por um grupo especifico de funções.
Uma abordagem alternativa à distribuição vertical, bastante comum nas arquiteturas modernas, se baseia na distribuição dos clientes e dos servidores, o que se chama de distribuição horizontal. Nessa distribuição, um cliente ou um servidor podem estar fisicamente divididos em partes logicamente equivalentes, onde cada uma opera sobre a sua própria porção dos dados, o que balanceia a carga. Os sistemas peer-to-peer se baseiam na distribuição horizontal.
Olhando de uma perspectiva de alto nível, os processos que constituem um sistema peer-to-peer são todos iguais. Isso significa que as funções necessárias devem estar em todos os processos que constituem o sistema distribuído. Como consequência, a maior parte da interação entre os processos é simétrica: cada processo atua como um "cliente" e um "servidor" ao mesmo tempo.[2]
Sistemas peer-to-peer compartilham essas características:
A correta operação de sistemas P2P não depende da existência de um sistema de administração centralizado. Assim, sistemas P2P se confundem com sistemas descentralizados. Num sistema totalmente descentralizado, não só todos os hospedeiros são iguais, mas também não há hospedeiros com atribuições especiais, como administração e descoberta de serviços. Na prática, construir sistemas totalmente descentralizados pode se tornar difícil, o que faz os projetistas geralmente adotarem paradigmas híbridos na construção de aplicações P2P. O DNS por exemplo, é um protocolo peer-to-peer, porém com um senso embutido de hierarquia. Há outros exemplos de sistemas P2P no seu núcleo e com alguma organização semi-centralizada, como o Napster[3] e BitTorrent[4]
Em redes peer-to-peer, a heterogeneidade dos recursos envolvidos é uma preocupação que deve ser levada em conta durante o seu projeto. Computadores e conexões administrados por diferentes usuários e organizações não têm garantias de ficarem ligados, conectados ou sem falhas, o que os torna necessariamente recursos voláteis. Isso torna a disponibilidade dos nodos de uma rede peer-to-peer imprevisível. Essa imprevisibilidade não permite garantir acesso a recursos individuais, já que eles podem falhar. Para contornar isso, é possível lançar mão da técnica de replicação, diminuindo consideravelmente a probabilidade de falha ao acessar um objeto replicado. A replicação pode também tornar o sistema mais confiável se utilizada para neutralizar a ação de nodos maliciosos, que interceptam o sistema e corrompem os dados, através de técnicas de tolerância à falhas bizantinas.
Os sistemas centralizados são simples de implementar e gerenciar, entretanto são um gargalo em potencial, uma vez que o servidor central tem capacidade limitada e pode não suportar o aumento da demanda. Por outro lado, os sistemas descentralizados são escaláveis e robustos, mas isso demanda certa complexidade de implementação, principalmente nas questões de tolerância à falhas e descoberta de recursos. Muitos sistemas distribuídos combinam características das duas arquiteturas, parte do sistema no tradicional modelo cliente-servidor e outra parte peer-to-peer.
Estruturas híbridas são implantadas notavelmente em sistemas distribuídos colaborativos. A principal preocupação em muitos desses sistemas é como se juntar ao sistema, para o qual muitas vezes um esquema tradicional cliente-servidor é adotado. Uma vez que o nodo se junta ao sistema, ele pode utilizar um esquema totalmente descentralizado para colaboração. Um exemplo de servidor que utiliza essa abordagem é o BitTorrent.[4]
Para um usuário de um serviço de comunicação instantânea, a aplicação parece peer-to-peer ao enviar os dados diretamente ao amigo sendo contactado. Mas todos os serviços de comunicação instantânea possuem uma espécie de servidor por trás que facilita a comunicação entre os nodos. Por exemplo, o servidor mantém uma associação entre o nome do usuário e o seu endereço IP, grava mensagens quando o usuário está offline, e roteia mensagens para usuários que estão atrás de firewalls. Um sistema totalmente descentralizado de comunicação instantânea não iria funcionar na Internet de hoje, mas existem grandes vantagens de escalabilidade em permitir comunicação cliente-cliente, quando possível. Assim, grande parte dos sistemas utiliza um esquema de diretório centralizado enquanto a função é distribuída.
Para funcionar eficientemente, sistemas peer-to-peer devem se preocupar com os seguintes requisitos não-funcionais:[5]
Um dos objetivos das aplicações peer-to-peer é explorar os recursos de hardware de um grande número de hospedeiros conectados à Internet. Assim, essas aplicações devem ser projetadas de modo a suportar o acesso a milhões de objetos em dezenas ou centenas de milhares de hospedeiros.
Alcançado através da colocação aleatória de recursos, juntamente com a utilização de réplicas dos recursos mais utilizados.
A "distância de rede" entre os nós que interagem tem um impacto substancial na latência das interações individuais, como por exemplo, clientes requisitando acesso à recursos. A carga do tráfego da rede também é impactada por isso. As aplicações devem colocar os recursos perto dos nós que mais os utilizam.
A maioria dos sistemas peer-to-peer são constituídos de computadores hospedeiros que são livres para se juntar ou sair do sistema a qualquer hora. Além disso, os segmentos de rede utilizados não são gerenciados por alguma autoridade; nem possuem garantias de qualidade de serviço. Um grande desafio para os sistemas peer-to-peer é prover um sistema confiável apesar desses fatos. Quando novos hospedeiros se juntam, eles devem ser integrados ao sistema e a carga deve ser redistribuída para explorar esses novos recursos. Quando eles saem do sistema voluntariamente ou involuntariamente, o sistema deve detectar a partida deles, e redistribuir as suas cargas e os seus recursos.
Em sistemas de escala global com participantes de origens diversas, confiança deve ser construída com o uso de autenticação e mecanismos de criptografia para garantir a privacidade dos dados e da informação.
Anonimidade é uma preocupação legitima em muitas situações que demandam resistência à censura. Um requisito relacionado é que hospedeiros que guardam dados devem ser capazes de negar plausivelmente a responsabilidade sobre a posse e o suprimento deles. A utilização de um grande número de hospedeiros em sistemas peer-to-peer pode ser útil em alcançar essas propriedades.
Um conceito importante do paradigma P2P é a rede sobreposta, ou rede overlay. Na rede overlay, os nós são formados pelos processos e os enlaces são representados pelos possíveis canais de comunicação (que são tipicamente conexões TCP). No geral, um processo não pode se comunicar diretamente com outro processo arbitrário, mas só pode enviar mensagens através dos canais de comunicação disponíveis. Nesse sentido, cada nó possui um conjunto de vizinhos, que por sua vez possuirão outros conjuntos de vizinhos. A figura abaixo ilustra a rede sobreposta sobre a camada de rede (p.e. Internet). Observe que dois nós que são vizinhos na rede overlay não são necessariamente vizinhos na rede física.
A localização de nós e objetos é realizado na rede overlay através de um algoritmo de roteamento distribuído. Esse algoritmo é implementado sobre a camada de aplicação, não tendo nenhuma ligação com o roteamento implementado pelos roteadores da camada de rede. Através desse algoritmo que as requisições dos cliente são roteadas para um hospedeiro que possui o objeto pela qual a requisição está endereçada. Os objetos de interesse são colocados e relocados em qualquer nó na rede sem o envolvimento do cliente.
O roteamento garante que qualquer nó pode acessar qualquer objeto na rede overlay, explorando o conhecimento de cada nó na rede para localizar o objeto de destino. Sistemas peer-to-peer geralmente armazenam múltiplas réplicas de um mesmo objeto para garantir disponibilidade. Dessa forma, o algoritmo de roteamento mantém o mínimo de informação possível sobre a localização de todas as réplicas, e envia as requisições para o nó "vivo" (p.ex. não falho) mais próximo que mantém uma cópia do objeto relevante.
Existem basicamente dois tipos de redes sobrepostas: as redes estruturadas e as redes não-estruturadas. Esses dois tipos de rede são discutidos extensivamente em.[6]
Numa rede peer-to-peer estruturada, a rede sobreposta é construída através de um procedimento determinístico. O procedimento mais utilizado, de longe, é organizar os processos através de uma tabela hash distribuída (DHT). Num sistema baseado em DHTs, os dados recebem uma chave aleatória de um extenso espaço de identificadores, tipicamente um identificador de 128 ou 160 bits. Os nodos da rede também recebem um identificador do mesmo espaço. O grande desafio num sistema baseado em DHT é implementar um esquema eficiente e determinístico que mapeia unicamente a chave de um item para o identificador do nodo responsável pelo item desejado. A partir disso, é possível retornar o endereço de rede do nodo responsável pelo item desejado, que pode ser contactado diretamente.
Várias implementações e protocolos baseados em DHT foram desenvolvidos. Dentre eles, as mais populares são o Chord.[7] NodeWiz[8] é um serviço de descoberta em grids (GIS) que utiliza uma estrutura de árvores para organizar os seus nós na rede sobreposta. Para tal, cada nó na árvore é responsável por um subespaço do espaço completo dos atributos. Assim, é possível realizar eficientemente consultas por faixas de valores, ao contrário do que acontece na DHT, onde as consultas se dão por valores exatos.
Sistemas peer-to-peer não estruturados geralmente se baseiam em algoritmos aleatorizados para a construção da rede sobreposta. A ideia principal é que cada nó mantenha uma lista de vizinhos, que é construída mais ou menos de forma aleatória. Da mesma forma, se assume que os dados são colocados de forma aleatória nos nodos. Assim, quando um nó necessita localizar um item específico, a única coisa que pode fazer é inundar a rede com uma busca.[9] Uma desvantagem desse tipo de busca é que consultas podem não ser respondidas caso o cliente e o hospedeiro estejam muito afastados na rede. Isso acontece, devido a mecanismos que impedem que mensagens se propaguem indefinidamente na rede (p. ex. TTL). Outra desvantagem é que mecanismos de inundação geralmente causam grande tráfego de sinalização, o que, muitas vezes, torna esse tipo de busca lenta.
Muitos sistemas peer-to-peer não estruturados constroem redes sobrepostas que remetem a um grafo aleatório. O modelo básico é que cada nó mantém uma lista com c vizinhos, onde, idealmente cada um desses vizinhos representa um nodo escolhido aleatoriamente dentre o conjunto dos nodos "vivos". Essa lista de nós pode ser chamada de visão parcial.
Dentre os sistemas P2P que utilizam arquiteturas não-estruturadas podemos citar o BitTorrent[4] que constrói sua overlay de forma aleatória, as Content Delivery Networks (CDNs), e o Gnutella.
Inicialmente, as aplicações peer-to-peer surgiram monolíticas, ou seja, o programa precisava implementar seu próprio protocolo de comunicação peer-to-peer para permitir a interoperabilidade entre os nós constituintes do seu sistema em rede. Porém, além de um grande re-trabalho, estes esforços em requisitos não-funcionais das aplicações implicavam na impossibilidade de comunicação entre sistemas diferentes, mesmo que os serviços providos por eles fossem equivalentes. Por exemplo, arquivos compartilhados em sistemas como o Kazaa, eMule e Gnutella ficam acessíveis exclusivamente dentro de suas próprias redes, levando usuários a manterem instalados em suas máquinas clientes para cada um dos sistemas de compartilhamento de arquivos que pretenda usar.
Com a popularização deste tipo de aplicação, surgiu um esforço em prover plataformas para desenvolvimento de aplicações peer-to-peer, de tal maneira que estas possam comunicar-se entre si.
Entre elas, destacam-se o JXTA, o Windows peer-to-peer Networking e o XNap.
O JXTA e o Windows peer-to-peer Networking são especificações de protocolos peer-to-peer e de uma API para utilização dos serviços, sendo o primeiro com implementações em Java e em C.
O XNap provê, além de uma API de serviços peer-to-peer, também um framework para desenvolvimento das aplicações em si, incluindo recursos de interface gráfica com o usuário. Um framework peer-to-peer, portanto, vai além de uma plataforma para comunicação peer-to-peer, provendo serviços adicionais não necessariamente relacionados com a comunicação em si, mas indispensáveis para o desenvolvimento rápido de aplicações baseadas nesta arquitetura.
Outros exemplos de frameworks para desenvolvimento de aplicações peer-to-peer são o Oog (Duke University), o Lancaster´s P2P Framework (University of Lancaster) e o COPPEER (UFRJ), sendo os dois últimos abstrações construídas sobre o JXTA.
Kademlia é conceito de rede altamente descentralizada baseada em "nós" de rede. Os próprios usuários constituem a estrutura da rede dispensando servidores. Várias redes utilizam o conceito Kademlia.
A rede Overnet é uma espécie de eDonkey "paga". É preciso comprar o programa da empresa que a desenvolveu. É uma variante do eDonkey totalmente descentralizada e mais rápida seguindo o conceito Kademlia e foi a primeira implementação da mesma.
Rede open-source surgida no final de 2000 utilizada inicialmente por usuários do sistema Linux. Possui uma estrutura altamente descentralizada não havendo mesmo nenhum servidor central sequer. Os usuários constituem a estrutura da própria rede. Entre os programas que a utilizam, estão o BearShare, LimeWire, Azureus e agora o Shareaza.
Segundo projeto da rede Gnutella mas agora com servidores centrais optimizando buscas e o desempenho geral da rede. É conhecida principalmente no programa Shareaza. Recebeu críticas quando foi criada pelos criadores da Gnutella original.
Rede paralela do programa eMule introduzida pelo autor deste em 2004; é uma implementação fiel ao conceito Kademlia. Essa rede tinha por objetivo inicial oferecer mais fontes aos usuários do programa e mais tarde se tornar uma rede P2P completa.
OpenFT é um protocolo desenvolvido pelo projeto giFT. O nome "OpenFT" significa "Open FastTrack". Entretanto, o OpenFT é um protocolo completamente novo, apenas algumas poucas vieram do pouco que se sabia sobre a FastTrack quando o OpenFT foi desenvolvido. Assim como a FastTrack, o OpenFT é uma rede onde nodos enviam listas de arquivos compartilhados para outros nodos. Isso reduz o consumo de banda necessário para a pesquisa, entretanto, consumindo mais recursos do processador e mémoria nos nodos.
Projeto antigo da empresa de mesmo nome, o Audiogalaxy centralizava todo o seu acervo indexando-o em seu sítio oficialmente. Foi facilmente fechada por um processo judicial na Inglaterra. Era uma implementação de FTPs sendo mais superficial ao usuário.
Rede introduzida para trocas de músicas em 2000. Utiliza programa de mesmo nome. Caracteriza-se pelo fato de ter um grande número de arquivos raros, e principalmente música alternativa. O programa cliente tem uma interface simplificada, e permite a adição de usuários em uma hotlist, ou seja, uma lista de contatos que permite saber quando um usuário que tem arquivos relevantes está conectado à rede. Também há na rede SoulSeek um serviço de bate-papo (chat) parecido com o IRC, que possibilita uma melhor interação entre os usuários, que também podem criar seus próprios canais de chat.
O DNS (Domain Name System - Sistema de Nomes de Domínios) é um exemplo de sistema que mistura os conceitos de rede peer-to-peer com um modelo hierárquico de posse da informação. O mais incrível do DNS é quão bem ele tem escalado, dos poucos milhares de hospedeiros que ele foi projetado para suportar, em 1983, para as centenas de milhões de hospedeiros atualmente na Internet.
Os problemas encontrados pelas aplicações P2P atuais, tais como compartilhamento de arquivos, são os mesmos problemas que foram resolvidos pelo DNS há 10 ou 15 anos atrás. Assim, vários elementos-chave no projeto do DNS são replicados nos sistemas distribuídos atuais. Um elemento é que hospedeiros podem operar tanto como clientes quanto como servidores. O segundo elemento é um método natural de propagar as requisições de dados pela rede. A carga é naturalmente distribuída pela rede, tanto que qualquer servidor individual de nomes só precisa servir as demandas dos seus clientes e o espaço de nomes que ele gerencia.
BitTorrent[4] é um sistema de download de arquivos P2P. A ideia básica é que quando um usuário procura por um arquivo, ele baixa "pedaços" do arquivo de outros usuários até que o arquivo fique completo. Um importante objetivo de projeto foi garantir colaboração. Na maioria dos sistemas de compartilhamento de arquivo, uma fração significante dos usuários somente baixa os arquivos e contribuem perto de nada.[10] [11] [12] Para isso, um arquivo pode ser baixado somente quando o cliente que está baixando também está provendo conteúdo para alguém.
Infinit é uma tecnologia de compartilhamento de arquivos P2P originalmente desenvolvida por Julien Quintard, durante sua pesquisa como estudante de doutoramento (PhD), na University of Cambridge.[13] Ao contrário de arquitetura com base em servidores centrais Cloud Computing, os dados são transmitidos diretamente entre máquinas na forma peer-to-peer. Tem sido relatado que o serviço é mais rápido do que outras soluções, incluindo protocolo de transferência Apple: AirDrop.[14] Além disso, os arquivos não são armazenados no Cloud e são sempre previamente criptografados, ou seja, os arquivos são criptografados antes de deixar o computador do remetente e só é capaz de decifrar o receptor.[15]
Os desenvolvedores do Napster[3] argumentaram que eles não eram culpados pelo infringimento dos direitos autorais porque eles não participavam do processo de cópia, que foi inteiramente realizado por máquinas de usuários. Esse argumento foi derrubado porque os servidores de indexação foram anexados como parte essencial do processo. Como esses servidores eram localizados em endereços conhecidos, os seus operadores foram incapazes de se manterem anônimos e então se tornaram alvos dos processos.
Um sistema de compartilhamento mais distribuído teria alcançado uma maior separação legal de responsabilidades, distribuindo a responsabilidade entre todos os usuários do Napster,[3] e tornando o processo muito difícil, senão impossível.
Qualquer que seja a visão que alguém tenha sobre a legitimidade de cópia de arquivos para o propósito de compartilhamento de material protegido por direitos autorais, existe uma legítima justificativa social e política para a anonimidade de clientes e servidores em alguns contextos de aplicações. A justificativa mais persuasiva é usada quando anonimidade é utilizada para superar censura e manter a liberdade de expressão para indivíduos em sociedades e organizações opressivas.[16]